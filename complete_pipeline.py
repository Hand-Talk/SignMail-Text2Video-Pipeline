import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# Add paths to all project directories
CURRENT_DIR = Path(__file__).parent.absolute()
TEXT2SIGN_DIR = CURRENT_DIR / "AST-Avatar-SignMail-Text2Video-main"
GENERATION_DIR = CURRENT_DIR / "ailab_Generation_pipeline-dev"
REALISDANCE_DIR = CURRENT_DIR / "realisDance_generation-my-feature-branch"
DWPOSE_DIR = GENERATION_DIR / "ailab_DWPose_not_git" / "ControlNet-v1-1-nightly"

# Create __init__.py files if they don't exist
def ensure_init_files():
    paths = [
        DWPOSE_DIR / "annotator" / "__init__.py",
        DWPOSE_DIR / "annotator" / "dwpose" / "__init__.py"
    ]
    for path in paths:
        path.parent.mkdir(parents=True, exist_ok=True)
        if not path.exists():
            path.touch()

# Ensure the paths exist and create __init__.py files
if not DWPOSE_DIR.exists():
    raise FileNotFoundError(f"DWPose directory not found at: {DWPOSE_DIR}")
ensure_init_files()

# Add paths to Python path (DWPose first, then other directories)
sys.path.insert(0, str(DWPOSE_DIR))
sys.path.extend([str(path) for path in [TEXT2SIGN_DIR, GENERATION_DIR, REALISDANCE_DIR]])

# Print debug information
print("Python path:", sys.path)
print("Current directory:", os.getcwd())
print("DWPose path:", DWPOSE_DIR)

# Now import main
import main as generation_pipeline

# Import required modules from all projects
from Text2SignID_run import text_to_signid as process_text_to_signID
from generate_single import generate_video as realisdance_generate

def setup_environment():
    """Setup necessary environment and configurations"""
    # Load environment variables (for OpenAI API key)
    load_dotenv()
    
    # Check if OpenAI API key is set
    if not os.getenv("OPENAI_API_KEY"):
        raise EnvironmentError("Please set OPENAI_API_KEY in your .env file")

def process_text_to_video(
    input_text, 
    output_video_path="./output_video.mp4", 
    dictionary="rdp", 
    num_interpolation=8, 
    style_image_path=None,
    render_realistic=True,
    pretrained_model_path=None,
    pretrained_clip_path=None,
    unet_checkpoint_path=None,
    reference_image_path=None
):
    """
    Process text input to generate sign language video
    
    Args:
        input_text (str): Input text to be converted to sign language
        output_video_path (str): Path where the output video will be saved
        dictionary (str): Which sign dictionary to use ('rdp' or 'ht')
        num_interpolation (int): Number of frames to interpolate between videos
        style_image_path (str): Optional path to style image for normalization
        render_realistic (bool): Whether to render realistic video using realisDance
        pretrained_model_path (str): Path to pretrained stable diffusion model
        pretrained_clip_path (str): Path to pretrained CLIP model
        unet_checkpoint_path (str): Path to realisDance model checkpoint
        reference_image_path (str): Path to reference image for realistic rendering
    """
    print("Step 1: Converting text to SignIDs...")
    # Process text to get SignIDs using the text_to_signid function
    df_result = process_text_to_signID(input_text, "temp_output.csv", dictionary)
    
    # Extract sign IDs from the DataFrame
    if not df_result.empty and 'id_sentence' in df_result.columns:
        sign_ids = df_result['id_sentence'].iloc[0].split()  # Get first row's sign IDs
    else:
        raise ValueError("No sign IDs were generated from the input text")
    
    print("Step 2: Generating pose-based video from SignIDs...")
    # Prepare arguments for generation pipeline
    generation_args = {
        "sign_ids": sign_ids,
        "num_interpolation": num_interpolation,
        "num_insert_interpolation": 0,  # Default value
        "style_image_path": style_image_path
    }
    
    # Run generation pipeline
    pose_video_path = "temp_pose_video.mp4"
    generation_pipeline.main(generation_args)
    
    if render_realistic and all([
        pretrained_model_path,
        pretrained_clip_path,
        unet_checkpoint_path,
        reference_image_path
    ]):
        print("Step 3: Rendering realistic video using realisDance...")
        # Extract keypoints from pose video (assuming they're saved during generation)
        keypoint_path = "temp_keypoints.npy"  # This should be generated by the pose pipeline
        
        # Generate realistic video
        realisdance_generate(
            keypoint_path=keypoint_path,
            output_path=output_video_path,
            pretrained_model_path=pretrained_model_path,
            pretrained_clip_path=pretrained_clip_path,
            unet_checkpoint_path=unet_checkpoint_path,
            reference_image_path=reference_image_path
        )
        print(f"Realistic video rendering completed!")
    else:
        if render_realistic:
            print("Warning: Skipping realistic rendering due to missing model paths")
        # Use the pose-based video as final output
        if os.path.exists(pose_video_path):
            os.rename(pose_video_path, output_video_path)
    
    # Clean up temporary files
    for temp_file in ["temp_output.csv", "temp_pose_video.mp4", "temp_keypoints.npy"]:
        if os.path.exists(temp_file):
            os.remove(temp_file)
    
    print(f"Process completed! Output video saved to: {output_video_path}")

def main():
    """Main function to run the complete pipeline"""
    setup_environment()
    
    # Example usage
    input_text = "Hello! This is a test message. How are you doing today?"
    
    try:
        process_text_to_video(
            input_text=input_text,
            output_video_path="./output_video.mp4",
            dictionary="rdp",
            num_interpolation=8,
            style_image_path=None,  # Optional: provide path to style image if needed
            render_realistic=False  # Disable realistic rendering since we don't have the model paths
        )
    except Exception as e:
        print(f"Error occurred: {str(e)}")

if __name__ == "__main__":
    main() 



#python -c "from complete_pipeline import process_text_to_video; process_text_to_video(input_text='Hello! How are you?', output_video_path='./output_video.mp4', dictionary='rdp', num_interpolation=8)"
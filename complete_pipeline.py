import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# Add paths to all project directories
CURRENT_DIR = Path(__file__).parent.absolute()
TEXT2SIGN_DIR = CURRENT_DIR / "AST-Avatar-SignMail-Text2Video-main"
GENERATION_DIR = CURRENT_DIR / "ailab_Generation_pipeline-dev"
REALISDANCE_DIR = CURRENT_DIR / "realisDance_generation-my-feature-branch"

sys.path.extend([str(TEXT2SIGN_DIR), str(GENERATION_DIR), str(REALISDANCE_DIR)])

# Import required modules from all projects
from Text2SignID_run import process_text_to_signID
import main as generation_pipeline
from generate_single import generate_video as realisdance_generate

def setup_environment():
    """Setup necessary environment and configurations"""
    # Load environment variables (for OpenAI API key)
    load_dotenv()
    
    # Check if OpenAI API key is set
    if not os.getenv("OPENAI_API_KEY"):
        raise EnvironmentError("Please set OPENAI_API_KEY in your .env file")

def process_text_to_video(
    input_text, 
    output_video_path="./output_video.mp4", 
    dictionary="rdp", 
    num_interpolation=8, 
    style_image_path=None,
    render_realistic=True,
    pretrained_model_path=None,
    pretrained_clip_path=None,
    unet_checkpoint_path=None,
    reference_image_path=None
):
    """
    Process text input to generate sign language video
    
    Args:
        input_text (str): Input text to be converted to sign language
        output_video_path (str): Path where the output video will be saved
        dictionary (str): Which sign dictionary to use ('rdp' or 'ht')
        num_interpolation (int): Number of frames to interpolate between videos
        style_image_path (str): Optional path to style image for normalization
        render_realistic (bool): Whether to render realistic video using realisDance
        pretrained_model_path (str): Path to pretrained stable diffusion model
        pretrained_clip_path (str): Path to pretrained CLIP model
        unet_checkpoint_path (str): Path to realisDance model checkpoint
        reference_image_path (str): Path to reference image for realistic rendering
    """
    print("Step 1: Converting text to SignIDs...")
    # Create a temporary file for the input text
    with open("temp_input.txt", "w", encoding="utf-8") as f:
        f.write(input_text)
    
    # Process text to get SignIDs
    output_csv = "temp_output.csv"
    sign_ids = process_text_to_signID("temp_input.txt", output_csv, dictionary)
    
    print("Step 2: Generating pose-based video from SignIDs...")
    # Prepare arguments for generation pipeline
    args = {
        "sign_ids": sign_ids,
        "num_interpolation": num_interpolation,
        "num_insert_interpolation": 0,  # Default value
        "style_image_path": style_image_path
    }
    
    # Run generation pipeline
    pose_video_path = "temp_pose_video.mp4"
    generation_pipeline.main(args)
    
    if render_realistic and all([
        pretrained_model_path,
        pretrained_clip_path,
        unet_checkpoint_path,
        reference_image_path
    ]):
        print("Step 3: Rendering realistic video using realisDance...")
        # Extract keypoints from pose video (assuming they're saved during generation)
        keypoint_path = "temp_keypoints.npy"  # This should be generated by the pose pipeline
        
        # Generate realistic video
        realisdance_generate(
            keypoint_path=keypoint_path,
            output_path=output_video_path,
            pretrained_model_path=pretrained_model_path,
            pretrained_clip_path=pretrained_clip_path,
            unet_checkpoint_path=unet_checkpoint_path,
            reference_image_path=reference_image_path
        )
        print(f"Realistic video rendering completed!")
    else:
        if render_realistic:
            print("Warning: Skipping realistic rendering due to missing model paths")
        # Use the pose-based video as final output
        if os.path.exists(pose_video_path):
            os.rename(pose_video_path, output_video_path)
    
    # Clean up temporary files
    for temp_file in ["temp_input.txt", "temp_output.csv", "temp_pose_video.mp4", "temp_keypoints.npy"]:
        if os.path.exists(temp_file):
            os.remove(temp_file)
    
    print(f"Process completed! Output video saved to: {output_video_path}")

def main():
    """Main function to run the complete pipeline"""
    setup_environment()
    
    # Example usage
    input_text = """
    Hello! This is a test message. How are you doing today?
    """
    
    try:
        process_text_to_video(
            input_text=input_text,
            output_video_path="./output_video.mp4",
            dictionary="rdp",
            num_interpolation=8,
            style_image_path=None,  # Optional: provide path to style image if needed
            render_realistic=True,  # Enable realistic rendering
            pretrained_model_path="path/to/stable-diffusion-model",  # Required for realistic rendering
            pretrained_clip_path="path/to/clip-model",  # Required for realistic rendering
            unet_checkpoint_path="path/to/realisdance-checkpoint",  # Required for realistic rendering
            reference_image_path="path/to/reference-image"  # Required for realistic rendering
        )
    except Exception as e:
        print(f"Error occurred: {str(e)}")

if __name__ == "__main__":
    main() 